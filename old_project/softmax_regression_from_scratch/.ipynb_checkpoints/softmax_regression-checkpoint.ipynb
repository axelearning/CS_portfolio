{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression from scratch\n",
    "---\n",
    "â†³ Implement Batch Gradient Descent with early stopping for Softmax Regression (without using Scikit-Learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What I Learned ?\n",
    "---\n",
    "- deeper understanding of gradient descent alogrithm\n",
    "- understand the goal of each hyperparameters and gain insight to tune them\n",
    "- learning to transform math equation to python code\n",
    "- handeling matrix multiplication with numpy\n",
    "- implementing different regularization algoritm \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "1. Load and preprocess the MNIST dataset\n",
    "2. Softmax Regression Model\n",
    "3. Ridge Regularization ($l_2$)\n",
    "4. Early Stoping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess the MNIST dataset\n",
    "---\n",
    "We made the choice to train our model on the __MNIST dataset__, a dataset composed of hand-written digits. This is a personal choice and the data are just use to implement the algorithm from scratch. Feel free to train your model on an other dataset.\n",
    "\n",
    " First, let's fetch the mnist dataset from openml. We will use Scikit-Learn `fetch_openml` function. _Note : it will be the first and last we will use Scikit-Learn in this notebook._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml # the first and last time we will use Scikit learn in the notebook\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize and Add the Bias Terme\n",
    "We normalize the pixel between 0 and 1, and had the bias term for every instance ($\\theta_0=1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist['data']/255\n",
    "X_with_bias = np.c_[np.ones([len(X), 1]), X]\n",
    "y = mnist['target'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function bellow displays instance from the dataset. Feel free to explore the dataset a little bit more by using different index on the function `show_image`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(x):\n",
    "    plt.imshow(x.reshape(28,28), cmap=mpl.cm.binary)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "show_image(X[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the few first instance of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 4\n",
    "columns = 4\n",
    "for i in range(rows):\n",
    "    for j in range(columns):\n",
    "        index = (rows * i + j )+ 1\n",
    "        plt.subplot(4,4,index)\n",
    "        show_image(X[index])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now __split the dataset into a training set, a validation set and a testing set__. As we can not use Scikit-learn, we will handle the splitting process with numpy and vanilla python. Instead of just using the method from Scikit learn `test_train_split` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ratio = 0.2\n",
    "testing_ratio = 0.2\n",
    "total_size = len(X) # number of instances\n",
    "\n",
    "validation_size = int(validation_ratio * total_size)\n",
    "testing_size = int(testing_ratio * total_size)\n",
    "training_size = len(X) - validation_size - testing_size\n",
    "\n",
    "random_index = np.random.permutation(total_size) \n",
    "\n",
    "X_train = X_with_bias[random_index[:training_size]]\n",
    "y_train = y[random_index[:training_size]]\n",
    "X_test = X_with_bias[random_index[training_size:-testing_size]]\n",
    "y_test = y[random_index[training_size:-testing_size]]\n",
    "X_val = X_with_bias[random_index[-validation_size:]]\n",
    "y_val = y[random_index[-validation_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"training size is composed of {X_train.shape[0]} instances\")\n",
    "print(f\"testing size: {X_test.shape[0]}\")\n",
    "print(f\"validation size: {X_val.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the Target into Binary Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target are store as a class indices, an array-like of integers between 0 and 9. To be handled by the Softmax Regression algorithm we need to __transform the target into a binary column of target class probabilites__. Look at the example bellow to have a better understanding of the transformation.  \n",
    "\n",
    "Let's create a simple function to handle it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  [2,3,...,6] â†’ [[1,0,..,0],[0,1,..,0],..[0,0,..,0]]\n",
    "def to_one_hot(y):\n",
    "    n_classes = 10\n",
    "    m = len(y)\n",
    "    Y_one_hot = np.zeros((m,n_classes))\n",
    "    Y_one_hot[np.arange(m),y] = 1\n",
    "    return Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array-like integer\n",
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# binary columns\n",
    "to_one_hot(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = to_one_hot(y_train)\n",
    "y_test_one_hot = to_one_hot(y_test)\n",
    "y_val_one_hot = to_one_hot(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¥³ The data are load and preprocess, ready to be used. Let's now create the Softmax Regression model and train the data on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Softmax Regression model\n",
    "---\n",
    "__How to make prediction with the Softmax Regression model ?__\n",
    "\n",
    "When given an instance $x$ to the Softmax Regression model, it computes a score $s_k(x)$ for each class k, then estimate the probability of each class by applying the _softmax function_ to the scores. Then, the Softmax Regression classifier predicts the class with the highest estimated probabilty.\n",
    "\n",
    "_1. Softmax score for each class_\n",
    "$$ s_k(x) = (\\theta^{(k)})^T x$$\n",
    "\n",
    "_2. Softmax function_\n",
    "$$ \\hat{p}_k = \\dfrac{exp(s_k(x))}{\\sideset{}{_{j=1}^K}\\sum exp(s_j(x))}$$\n",
    "\n",
    "_3. Classifier prediction_\n",
    "$$ \\hat{y} = \\underset{k}{argmax }\\, \\sigma(s(x))_k = \\underset{k}{argmax}\\, (s(x))_k = \\underset{k}{argmax}\\, (\\theta^{(k)})^T x$$\n",
    "\n",
    "---\n",
    "\n",
    "First, in the purpose to create a clean training loop, let's define a softmax function which take the score of each class k and transform it into a probability. Note that the some of the probabilty class is equal to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(score):\n",
    "    \"\"\"\n",
    "    output: the prediction proba of each features for each class \n",
    "    \"\"\"\n",
    "    exps = np.exp(score)\n",
    "    exps_sum = np.sum(exps,axis=1,keepdims=True)\n",
    "    return exps / exps_sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = X_train.shape[1] # number of features \n",
    "n_outputs = y_train_one_hot.shape[1] # number of classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The training loop__: we need to translate the math equation into python code. \n",
    "Here is the equations we will need in the training loop\n",
    "- _the cost function_\n",
    "$J(\\mathbf{\\Theta}) =\\dfrac{1}{m}\\sum\\limits{_{i=1}^m}^{}\\sum\\limits{_{k=1}^K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}$\n",
    "- _the equation for the gradient_\n",
    "$\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}$\n",
    "\n",
    "Note that $\\log\\left(\\hat{p}_k^{(i)}\\right)$ may not be computable if $\\hat{p}_k^{(i)} = 0$. So we will add a tiny value $\\epsilon$ to $\\log\\left(\\hat{p}_k^{(i)}\\right)$ to avoid getting nan values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Math equation to Python code</b>\n",
    "    \n",
    "All the math equation write above are made for a single instance on a single class. However, when we translate it to python code we need to transform each equations. The goal is to make them handle the full training set in one computation. That why this part can tricky and it could be usefull to write the shape of each vectors and matrix on a white sheet to be sure we compute the thing we care about. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "eta = 0.01 # learning rate\n",
    "n_epochs = 5001 \n",
    "m = X_train.shape[0] # number of instances\n",
    "epsilon = 1e-7\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs) # initialize the parameter matrix Î˜\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    score = X_train.dot(Theta)\n",
    "    y_proba = softmax(score)\n",
    "    loss = - np.mean(np.sum(y_train_one_hot * np.log(y_proba + epsilon), axis=1)) # cost function\n",
    "    error = y_proba - y_train_one_hot\n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch,loss)\n",
    "    gradient = 1/m * X_train.T.dot(error)\n",
    "    Theta = Theta - eta * gradient # GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¥³ The Softmax model is train, let's look at the parameters matrix Î˜:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make predictions for the validation set and check the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = X_val.dot(Theta)\n",
    "y_proba = softmax(score)\n",
    "y_predict = np.argmax(y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_val)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad the model are now trained! Let's try now to add some regularization and implement early stopping as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ridge regularization $(l_2)$\n",
    "---\n",
    "The code bellow is pretty much the same than the one above except that we add a regularize term in the loss function. We have also increased the learning rate \n",
    "\n",
    "$J(\\mathbf{\\Theta}) =$ _Cross entropy loss_ $ + \\dfrac{1}{2}\\alpha\\sum{_{i=1}^n}^{}\\sum{_{k=1}^K}{\\theta_i^k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "eta = 0.1 # learning rate\n",
    "n_epochs = 5001 \n",
    "m = X_train.shape[0] # number of instances\n",
    "epsilon = 1e-7\n",
    "alpha = 0.1 # regularized hyperparameter\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs) # initialize the parameter matrix Î˜\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    score = X_train.dot(Theta)\n",
    "    Y_proba = softmax(score)\n",
    "    x_entropy_loss = - np.mean(np.sum(y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    l2_loss = 1/2 * np.sum(np.square(Theta[1:])) # we don't regularize the bias\n",
    "    loss = x_entropy_loss + alpha * l2_loss\n",
    "    error = Y_proba - y_train_one_hot\n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch,loss)\n",
    "    gradient = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
    "    Theta = Theta - eta * gradient # GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make predictions for the validation set and check the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = X_train.dot(Theta)\n",
    "y_proba = softmax(score)\n",
    "y_predict = np.argmax(y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_train)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Early Stoping\n",
    "---\n",
    "We need to mesure the loss on the validation set at each epoch and stop the model when the loss start growing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "eta = 0.1 # learning rate\n",
    "n_epochs = 10001 \n",
    "m = X_train.shape[0] # number of instances\n",
    "epsilon = 1e-7\n",
    "alpha = 0.1 # regularized hyperparameter\n",
    "best_loss = np.infty\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs) # initialize the parameter matrix Î˜\n",
    "\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    score = X_train.dot(Theta)\n",
    "    Y_proba = softmax(score)\n",
    "    x_entropy_loss = - np.mean(np.sum(y_train_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    l2_loss = 1/2 * np.sum(np.square(Theta[1:])) # we don't regularize the bias\n",
    "    loss = x_entropy_loss + alpha * l2_loss\n",
    "    error = Y_proba - y_train_one_hot\n",
    "    if epoch % 500 == 0:\n",
    "        print(epoch,loss)\n",
    "    gradient = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
    "    Theta = Theta - eta * gradient # GD\n",
    "    \n",
    "    # add early stoppingval loss\n",
    "    score = X_val.dot(Theta)\n",
    "    Y_proba = softmax(score)\n",
    "    x_entropy_loss = - np.mean(np.sum(y_val_one_hot * np.log(Y_proba + epsilon), axis=1))\n",
    "    l2_loss = 1/2 * np.sum(np.square(Theta[1:])) # we don't regularize the bias\n",
    "    val_loss = x_entropy_loss + alpha * l2_loss\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "    else:\n",
    "        print(epoch - 1, best_loss)\n",
    "        print(epoch, loss, \"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = X_train.dot(Theta)\n",
    "y_proba = softmax(score)\n",
    "y_predict = np.argmax(y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_train)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
